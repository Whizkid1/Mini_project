{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "edce4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "from urllib.request import urlopen\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import GetOldTweets3 \n",
    "import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a4fa34ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJsonResponse(tweetCriteria, refreshCursor, cookieJar, proxy, useragent=None, debug=False):\n",
    "    url = \"https://twitter.com/i/search/timeline?\"\n",
    "\n",
    "    if not tweetCriteria.topTweets:\n",
    "        url += \"f=tweets&\"\n",
    "\n",
    "    url += (\"vertical=news&q=%s&src=typd&%s\"\n",
    "            \"&include_available_features=1&include_entities=1&max_position=%s\"\n",
    "            \"&reset_error_state=false\")\n",
    "\n",
    "    urlGetData = ''\n",
    "    \n",
    "    # url + query search, since, since, until, lang    \n",
    "    urlGetData += tweetCriteria.querySearch\n",
    "    urlGetData += ' since:' + tweetCriteria.since\n",
    "    urlGetData += ' until:' + tweetCriteria.until\n",
    "    urlLang = 'l=' + tweetCriteria.lang + '&'\n",
    "    \n",
    "    # url 설정\n",
    "    url = url % (urllib.parse.quote(urlGetData.strip()), urlLang, urllib.parse.quote(refreshCursor))\n",
    "    useragent = useragent or TweetManager.user_agents[0]\n",
    "\n",
    "    headers = [\n",
    "        ('Host', \"twitter.com\"),\n",
    "        ('User-Agent', useragent),\n",
    "        ('Accept', \"application/json, text/javascript, */*; q=0.01\"),\n",
    "        ('Accept-Language', \"en-US,en;q=0.5\"),\n",
    "        ('X-Requested-With', \"XMLHttpRequest\"),\n",
    "        ('Referer', url),\n",
    "        ('Connection', \"keep-alive\")\n",
    "    ]\n",
    "\n",
    "    if proxy:\n",
    "        opener = urllib.request.build_opener(urllib.request.ProxyHandler({'http': proxy, 'https': proxy}), urllib.request.HTTPCookieProcessor(cookieJar))\n",
    "    else:\n",
    "        opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookieJar))\n",
    "    opener.addheaders = headers        \n",
    "    \n",
    "    time.sleep(1) # HTTP Request 429 에러 방지.\n",
    "\n",
    "    ##################### 에러 핸들링 수정 #####################\n",
    "    \n",
    "    # TimeOut 에러\n",
    "    try:\n",
    "        response = opener.open(url)\n",
    "        jsonResponse = response.read()\n",
    "    except TimeoutError as e:\n",
    "        if debug: # 디버그 옵션\n",
    "            print(\"에러 url 주소:\", url)            \n",
    "        print(\"Timeout error\")\n",
    "        print(\"30초 정지\")\n",
    "        time.sleep(30)\n",
    "\n",
    "        # 한 번 더 시도\n",
    "        try:\n",
    "            response = opener.open(url)\n",
    "            jsonResponse = response.read()\n",
    "        except TimeoutError as e:\n",
    "            print(\"Timeout error again. 패스.\")\n",
    "            pass\n",
    "\n",
    "    # UrlParse 에러: 다시 시도 X\n",
    "    except Exception as e:\n",
    "        if debug: # 디버그 옵션\n",
    "            print(\"에러 url 주소:\", url)\n",
    "            \n",
    "        print(\"HTTP 요청 오류\", str(e))        \n",
    "        print(\"브라우저 오픈: https://twitter.com/search?q=%s&src=typd\" % urllib.parse.quote(urlGetData))\n",
    "        print(\"30초 정지\")\n",
    "        \n",
    "        pass\n",
    "\n",
    "    # Json 데이터 오류\n",
    "    try:\n",
    "        s_json = jsonResponse.decode()\n",
    "    except:\n",
    "        print(\"올바르지 못한 응답\")\n",
    "        if debug: # 디버그 옵션\n",
    "            print(\"에러 url 주소:\", url)\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            dataJson = json.loads(s_json)\n",
    "        except: # json 데이터 파싱 오류\n",
    "            if debug: # 디버그 옵션\n",
    "                print(\"에러 url 주소:\", url)                \n",
    "            print(\"JSON: %s\" % s_json)\n",
    "        pass\n",
    "\n",
    "    return dataJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "485f3aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateRange = pd.date_range(start='20120601', end='20120701', freq='MS')\n",
    "\n",
    "# setUntil : 마지막 날짜 배제되므로 주의.\n",
    "def set_crawl_date(start_date, freq=1):    \n",
    "    end_date = start_date + datetime.timedelta(days=freq)\n",
    "    \n",
    "    # timestamp to string format\n",
    "    start_date = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_date = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # check\n",
    "    print(\"트윗 수집 날짜 설정: {0}부터 {1}까지\".format(start_date, end_date)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e15d0380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_tweets(start_date, end_date, 박근혜, lang='ko', debug=True):    \n",
    "    '''\n",
    "    query: 검색할 트윗 검색어\n",
    "    lang: 검색할 트윗 언어\n",
    "    debug: 설정 시 에러 url 표시\n",
    "    '''\n",
    "    \n",
    "    print(\"========== 트윗 수집 시작: {0} ~ {1} ==========\".format(start_date, end_date))\n",
    "    start_time = time.time()\n",
    "    tweet_criteria = got.manager.TweetCriteria().setQuerySearch(query)\\\n",
    "                                                .setSince(start_date)\\\n",
    "                                                .setUntil(end_date)\\\n",
    "                                                .setLang(lang)\n",
    "    tweets = got.manager.TweetManager.getTweets(tweet_criteria, debug=debug)\n",
    "    \n",
    "    elapsed_time = time.time()-start_time\n",
    "    \n",
    "    print(\"수집 완료 : {}\".format(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))\n",
    "    print(\"총 수집 트윗 개수 : {0}\".format(len(tweets)))\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "531319b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(tweet_data):\n",
    "    results = []\n",
    "    for tweet in tqdm(tweet_data):\n",
    "        results.append({'url': tweet.permalink,\n",
    "                        'date': tweet.date,\n",
    "                        'text': tweet.text,\n",
    "                        'user': tweet.username,\n",
    "                        'mentions': tweet.mentions,\n",
    "                        'retweets': tweet.retweets,\n",
    "                        'favorites': tweet.favorites,\n",
    "                        'hashtags': tweet.hashtags})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "824f9ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tweets(tweet_lists, base_file_dir=\"tweets\"):\n",
    "    \n",
    "    if not os.path.exists(base_file_dir):\n",
    "        os.makedirs(base_file_dir)\n",
    "        \n",
    "    with open(f\"{base_file_dir}/tweets_{crawl_start}_{crawl_end}.csv\", \"a\", -1, encoding=\"utf-8\") as f:    \n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['url', 'date', 'text', 'user', 'mentions', 'retweets', 'favorites', 'hashtags'])        \n",
    "        for tweet_list in tqdm(tweet_lists):\n",
    "            writer.writerow(list(tweet_list.values()))\n",
    "            \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "85c163ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'datetime.datetime' has no attribute 'timedelta'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-98fba62f671e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdateRange\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mcrawl_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrawl_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset_crawl_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# freq 변경 가능\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtweet_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrawl_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrawl_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrawl_end\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SomeString'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtweet_results_lists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-110-da2dfdb3c0e4>\u001b[0m in \u001b[0;36mset_crawl_date\u001b[1;34m(start_date, freq)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# setUntil : 마지막 날짜 배제되므로 주의.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mset_crawl_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mend_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart_date\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# timestamp to string format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'datetime.datetime' has no attribute 'timedelta'"
     ]
    }
   ],
   "source": [
    "dateRange = pd.date_range(start='20120601', end='20120701', freq='MS')\n",
    "\n",
    "for date in dateRange:\n",
    "    crawl_start, crawl_end = set_crawl_date(date) # freq 변경 가능\n",
    "    tweet_results = crawl_tweets(crawl_start, crawl_end, query='SomeString')\n",
    "    tweet_results_lists = get_results(tweet_results)\n",
    "    save_tweets(tweet_results_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf4a56a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a640ca46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
